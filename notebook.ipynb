{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Group 11\n\n## Iben Mai Huse, Karlis Buiko, Markus Sibbesen, Ida Kathrine Jensen",
      "metadata": {
        "tags": [],
        "cell_id": "00000-fde8f23b-a8ff-4a2a-aa40-8aeaf7c6ea2d",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "This notebook contains the code for data exploration, and will be the hand-in for the First Year Project 4\n\nCreated 04.05.2021\n\nLast modified, modified by:\n\n04.05.2021, Ida",
      "metadata": {
        "tags": [],
        "cell_id": "00001-96196863-3c10-4b9f-82ec-ecf208043f9a",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Imports",
      "metadata": {
        "tags": [],
        "cell_id": "00002-8c8740c1-2e4a-4087-bf8c-ccec04a3ffdd",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00000-add3736a-6dbe-424f-bed3-d8e3c41d9f2c",
        "deepnote_cell_type": "code"
      },
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Path to files",
      "metadata": {
        "tags": [],
        "cell_id": "00004-1ecb2c91-4974-4eb1-8815-ff9e30ea2df5",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00004-d3022775-00a7-42bb-86bd-dd445262d0ee",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Task 1: Preprocessing",
      "metadata": {
        "tags": [],
        "cell_id": "00006-e877412a-0991-45b0-b70a-c43564b532c7",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "#### We have selected *hate speech detection* as the binary classifier and the *sentiment analysis* as the multiclass classifier.",
      "metadata": {
        "tags": [],
        "cell_id": "00007-2214d8d1-86a5-4d11-8cb2-81d8e9b293bc",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "*Using regular expressions, implement a tokeniser to split the input texts into meaningful tokens. Your tokeniser should be a script that takes as input the data as distributed in the dataset and outputs the tokenised text, one output line per input line, with spaces between tokens.*",
      "metadata": {
        "tags": [],
        "cell_id": "00007-961b9d3a-d1dc-45b4-9bc1-06374420cf99",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "*• Use the training set of one of the tasks you’ve chosen to work on. Set aside a part of the training set for evaluating the tokeniser so you don’t have to touch the validation data for that.*",
      "metadata": {
        "tags": [],
        "cell_id": "00008-9ab3099e-6000-4e8d-858f-583beb76182b",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00004-3e1a99a1-b62b-45e2-9e24-861fd814ce53",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "*• Start by taking a subset of the trainingdata, look through it and discuss in your group what a good tokenisation of this subset should look like. Then design your initial Python implementation to match this ideal tokenisation, and run it over the portion of the training set that you didn’t hold out for tokeniser evaluation. Keep an eye on infrequently occurring tokens in the output that look like tokenisation errors.*",
      "metadata": {
        "tags": [],
        "cell_id": "00010-df5b7a5e-b940-49f0-b90e-4d5acffae875",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00005-f6acf72a-efe3-4baa-9377-46c2f3da9c99",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "*• Once you’re done, compare your tokeniser’s output with the baseline tokenisation\nyou get from the social media tokeniser in the NLTK library (nltk.tokenize.TweetTokenizer), using the data you’ve set aside for this purpose.*",
      "metadata": {
        "tags": [],
        "cell_id": "00012-74a6c041-800b-41d6-8ae5-9a8481744c4f",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00006-29e43166-5498-4517-b2d4-f9a239256fb0",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "*\n• Consider using the difflib package in Python, or the diff utility in the Unix shell, to compare the output of the two tokenisers efficiently.*",
      "metadata": {
        "tags": [],
        "cell_id": "00014-23083094-9a58-40ef-9480-983c9ed9d1dc",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00007-6b488828-5f55-4991-aa9f-dfca5dc698ec",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Task 2: Characterising Your Data",
      "metadata": {
        "tags": [],
        "cell_id": "00016-732de60c-ac86-4eb2-9537-542d2bd99250",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "*Characterise the training sets of the two tasks you’ve chosen in terms of elementary corpus statistics: <br>\n• Corpus size, vocabulary size,type/token ratio.<br>\n• What are the most frequent tokens?<br>\n• What types of tokens occur only once, or 2 or 3 times?<br>\n• Are there any noticeable differences between your two datasets?<br>\n• Are the corpus statistics consistent with Zipf’slaw?(no formal test needed,but a plot would be helpful)*<br>",
      "metadata": {
        "tags": [],
        "cell_id": "00017-3e3d86b3-a3eb-4657-9392-add13ccf37a0",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00017-27ff618b-a298-4a47-a3a4-62aeb245a507",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Task 3: Manual Annotation and Inter-Annotator Agreement",
      "metadata": {
        "tags": [],
        "cell_id": "00017-7ed94529-8efb-42e1-8a73-825d0b6fa5eb",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "*Choose one of your two datasets. For this subtask, the emoji prediction dataset doesn’t make sense, so if that is one of your choices, pick the other one for this part of the assignment.*",
      "metadata": {
        "tags": [],
        "cell_id": "00020-70c4397e-1bda-4990-b82a-799b45a72296",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "*In the README file in the TweetEval corpus repository, there are links to research papers from the SemEval workshop, describing how each of the datasets was created and annot- ated. Locate the one that belongs to the dataset you’ve picked for this subtask and find the passages that describe in detail how the labels for the dataset were created. Read these passages carefully.*",
      "metadata": {
        "tags": [],
        "cell_id": "00021-9ab1a68d-7e5b-468c-8f87-dac50da5c404",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "*Next, select a random sample of 100 tweets from the training set. Working independently from each other and without consulting the labels published in the TweetEval corpus, each member of your group should now go manually through this sample and label them ac- cording to the same scheme.*",
      "metadata": {
        "tags": [],
        "cell_id": "00022-9203f65c-1a7f-434e-9d07-8c065e55b179",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "*Report on the inter-annotator agreement, including the agreement with the published la- bels, and discuss what phenomena in the data caused the biggest problems for inter-an- notator agreement.*",
      "metadata": {
        "tags": [],
        "cell_id": "00023-fde6d355-ef4d-46d8-a767-14afffb468e2",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00021-87e27f76-efbc-4258-9143-05f2eba23b56",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Task 4: Automatic Prediction",
      "metadata": {
        "tags": [],
        "cell_id": "00021-c60d322e-a641-46f4-b2b0-9c460a4ec7b8",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "*Finally, use scikit-learn to train a classifier for the automatic prediction of the labels in the two datasets you have chosen. During the lessons, we have not had time to discuss ma- chine learning techniques and classification methods in detail, so in this exercise you will be using library implementations as “black box” methods.* <br>\n\n*Run all classification experiments on both of the tasks you’ve chosen (one binary and one multi-class task). Evaluate your different classifiers on the validation set and report relev- ant evaluation metrics (accuracy, precision/recall/F-score).* <br>\n\n*As a baseline, start with the sklearn.linear_model.SGDClassifier in a logistic regres- sion configuration (loss=’log’) using bag of words features. Then run at least additional experiments trying to improve your initial scores by any means you can think of. Try out at least 4 different methods. Usually you will need to run several experiments for each meth- ods to test different parameter values. Here are some settings in scikit-learn that you could experiment with:*",
      "metadata": {
        "tags": [],
        "cell_id": "00026-f7b991ea-912f-4854-b2b3-7fe0b015640b",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "*• Additional preprocessing options: n-gram features, lowercasing, stop word lists (see options to sklearn.feature_extraction.text.CountVectorizer)*. <br>\n\n*• Count transformations (sklearn.feature_extraction.text.TfIdfTransformer).* <br>\n\n*• The classification loss(lossparametertoSGDClassifier).* <br>\n\n*• The regularisation strength (alpha parameter to SGDClassifier – try varying it in exponentially spaced steps).* <br>\n\n*• Different classifiers (e.g.,sklearn.ensemble.Random Forest Classifier or sklearn.naive_bayes.MultinomialNB.* <br>\n\n*• Anything else discussed during the lessons,or implemented in scikit-learn.*",
      "metadata": {
        "tags": [],
        "cell_id": "00027-9ebbfd13-564b-4b4c-9fa6-14ea7bb1a14f",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "*(For the systems that achieves the highest accuracy on the validation set, run the evaluation on the test set and report your results. We will share an anonymous overview of the test set scores of all groups after the reports are handed in.)*",
      "metadata": {
        "tags": [],
        "cell_id": "00028-ed4f8ae2-0bac-402a-99e9-0fd0073bac46",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00027-2af8bb81-94fa-43a6-9563-7ec0b9819d8e",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00023-2351ca9a-9100-4fb3-a26d-8df47e552dbc",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=5235cf77-d347-437f-a336-60ef81766ce7' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_notebook_id": "7a186dc4-7f7b-4be7-ae14-975d1edff29c",
    "deepnote_execution_queue": []
  }
}